# general configs
agent:
  desc: method used for the agent
  value: sac
verbose:
  desc: prints intermediate results to sdt.out
  value: true
run_id:
  desc: dir name in base-dir. Logs are in here
  value: mixed_prec
agent_model_name:
  desc: if not empty, agent will be initialized with the state dict at this location
  value: '' # agent_model_svpfe.pth
data_dir:
  desc: train data dir
  value: /g/kreshuk/kaziakhm/leptin_data/processed/v4_dwtrsd/train
val_data_dir:
  desc: validation data dir
  value: /g/kreshuk/kaziakhm/leptin_data/processed/v4_dwtrsd/val
val_data_keys:
  desc: keys for data in h5 file
  value:
    raw: "raw"  # raw_2chnl
    raw_1: "ps_edge"
    gt: "gt"
    node_labeling: "node_labeling"
#    gt_edge_weights: "gt_edge_weights"
    edge_feat: "edge_feat"
    edges: "edges"
train_data_keys:
  desc: keys for data in h5 file
  value:
    raw: "raw"  # raw_2chnl
    raw_1: "ps_edge"
#    gt: "gt"
    node_labeling: "node_labeling"
#    gt_edge_weights: "gt_edge_weights"
    edge_feat: "edge_feat"
    edges: "edges"
patch_manager:
  desc: if training should happen on patches, this is the config for the patch manager
  value:
    name: none # rotated, no_cross, none
    reorder_sp: false

# embedding network config
fe_model_name:
  desc: path to state dict of trained model
  value: /g/kreshuk/kaziakhm/paper_result/fe/leptin_ds/fe_fds_005wf_psedge_16fs/best_val_model.pth # fe_cosine_ls_bn_2chnl.pth # fe_cosine_ls_bn_gtval, fe_cosine_ls_bn_2chnl
dim_embeddings:
  desc: number of embedding feature channels
  value: 16
distance:
  desc: used distance in embedding space, can be l2 or cosine
  value: cosine
use_handcrafted_features:
  desc: to embeddings append handcrafted features like sp size, position , orientation
  value: true
n_init_edge_feat:
  desc: GNN gets hand crafted edge featurs like mean affinity or embedding distance as well can be none if not used
  value: 4
fe_delta_dist:
  desc: delta distance of contrastive loss during embedding space training
  value: 0.4
backbone:
  desc: config for the embedding network
  value:
    name: UNet2D
    in_channels: 2
    out_channels: 16
    # use Groupnorm instead of Batchnorm for DSB; Batchnorm introduces artifacts around nuclei due to the difference
    # in intensity distribution between images with large and small cells
    layer_order: bcr
    num_groups: 8
    f_maps: [32, 64, 128]
    #conv_padding: 0
    final_sigmoid: false
    is_segmentation: false

gnn_n_hl:
  desc: number of hidden layers in graph conv blocks
  value: 1
gnn_size_hl:
  desc: factor by which feature size of hidden featuremaps in gnn increases/decreases
  value: 1024
gnn_act_depth:
  desc: num of graph convs (1, 2, 3)
  value: 1
gnn_act_norm_inp:
  desc: put hidden features on unit sphere and scale by sim
  value: false
gnn_crit_depth:
  desc: num of graph convs (1, 2)
  value: 1
gnn_crit_norm_inp:
  desc: put hidden features on unit sphere and scale by sim
  value: false
gnn_dropout:
  desc: dropout probability during training \in [0, 1]
  value: 0.2

# training config
T_max:
  desc: Number of training steps
  value: 50000
mem_size:
  desc: Mem size in replay memory buffer
  value: 50
data_update_frequency:
  desc: update env data after n steps
  value: 200
post_stats_frequency:
  desc: post logs after n steps
  value: 10
validatoin_freq:
  desc: validate after n steps
  value: 100
store_indices:
  desc: validate after n steps
  value: [1, 5, 9]
n_updates_per_step:
  desc: perform n optim steps per env step
  value: 10
n_explorers:
  desc: n experience explorer threads
  value: 1
episode_len:
  desc: length of the RL episode
  value: 3
temporal_encoding_len:
  desc: the current step is encoded by frequency shifted sine waves. This is the number of sines used
  value: 5
use_expact_in_expl:
  desc: if true we use the same action in training that was used in exploration where the reward was actually gained from.
  value: true
batch_size:
  desc: num samples in minibatch
  value: 4

# conf for lr scheduling
lr_sched:
  desc: config for moving averages and learning rate sheduling
  value:
    mov_avg_bandwidth: 40
    weight_range: [0.1, 1.0]
    step_frequency: 10
    mov_avg_offset: 0.0
    torch_sched:  # conf for torch reduceOnPlateau scheduler
      patience: 80
      min_lr: 0.0001
      factor: 0.05
      threshold: 0.01

# specific configs for the sac algorithm
reward_function:
  desc: one of sub_graph_dice, leptin_dataTurningFunc, leptin_dataRectFit, leptin_dataTurningWithEllipses
  value: leptin_dataRectFit
gt_edge_overlap_thresh:
  desc: if using gt edges this is the overlap thresh of the underlying gt for a gt edge to be 1
  value: 0.5
s_subgraph:
  desc: subgraph sizes
  value: [6, 64, 256] # [8, 16, 32, 64]
entropy_range:
  desc: range of the min entropy constraint
  value: [0.05, 1.0]
init_temperature:
  desc: initial temperature in Gibbs distribution
  value: 0.1
alpha_lr:
  desc: learning rate for alpha opt
  value: 0.001
actor_lr:
  desc: learning rate for actor opt
  value: 0.001
critic_lr:
  desc: learning rate for critic opr
  value: 0.001
actor_update_frequency:
  desc: optim every n-th step
  value: 1
actor_update_after:
  desc: update actor after critic has been warmed up
  value: 1
critic_tau:
  desc: update weight for target update
  value: 0.005
critic_target_update_frequency:
  desc: update critic target every nth step
  value: 5
gamma:
  desc: RL gamma
  value: 0.9

use_closed_form_entropy:
  desc: since we have normal distributions we can use a closed for entropy
  value: true
side_loss_weight:
  desc: loss weight for the edge-gnn side loss
  value: 0
# specification of multinomial gaussian, acting as policy
std_bounds:
  desc: norm bounds of predicted std dev
  value: [0.01, 5.0]
mu_bounds:
  desc: norm bounds of predicted mean
  value: [-5, 5]

#policy_warmup:
#  desc: the policy might need pretraining to make training faster
#    value:
#    lr: 1e-3
#
#    n_iterations: 500 # number of iterations of feature extrqactor warmup
#    batch_size: 1  # batch size for feature extractor warmup
#
#    patch_manager:
#      name: no_cross # rotated, no_cross, none
#      patch_shape: [512, 512]
#      patch_stride: [128, 128]
#      reorder_sp: true
