# general configs
agent:
  desc: method used for the agent
  value: sac
verbose:
  desc: prints intermediate results to sdt.out
  value: true
random_seed:
  desc: random seed
  value: 123456
agent_model_name:
  desc: if not empty, agent will be initialized with the weights at this location
  value: '' # agent_model_svpfe.pth
train_data_dir:
  desc: train data dir
  value: /g/kreshuk/hilt/projects/data/color_circles/all_in_one/train
val_data_dir:
  desc: validation data dir
  value: /g/kreshuk/hilt/projects/data/color_circles/all_in_one/val
train_data_keys:
  desc: keys for data in h5 file
  value:
    raw: "raw"
    gt: "gt"
    superpixels: "superpixels"
    gt_edge_weights: "gt_edge_weights"
    edge_feat: "edge_feat"
    node_feat: "node_feat"
    edges: "edges"
val_data_keys:
  desc: keys for data in h5 file
  value:
    raw: "raw"
    gt: "gt"
    superpixels: "superpixels"
    gt_edge_weights: "gt_edge_weights"
    edge_feat: "edge_feat"
    node_feat: "node_feat"
    edges: "edges"
patch_manager:
  desc: if training should happen on patches, this is the config for the patch manager
  value:
    name: none # rotated, no_cross, none
    reorder_sp: true

# embedding network config
fe_model_name:
  desc: path to state dict of trained model
  value: /g/kreshuk/hilt/storage/color_circles/fe_cosine.pth # fe_cosine_ls_bn_gtval, fe_cosine_ls_bn_2chnl
fe_optimization:
  desc: if true the fe will be optimized by the critic
  value: false
dim_embeddings:
  desc: number of embedding feature channels
  value: 16
distance:
  desc: used distance in embedding space, can be l2 or cosine
  value: cosine
use_handcrafted_node_features:
  desc: to embeddings append handcrafted features like sp size, position , orientation
  value: true
use_handcrafted_edge_features:
  desc: use handcrafted features like mean affinity, affinity-variance ...
  value: true
n_init_edge_feat:
  desc: additional GNN edge featurs like mean affinity or embedding distance as well can be none if not used
  value: 4
n_init_node_feat:
  desc: additional GNN node featurs like mean affinity or embedding distance as well can be none if not used
  value: 3
fe_delta_dist:
  desc: delta distance of contrastive loss during embedding space training (deprecated and not used anymore)
  value: 0.4
backbone:
  desc: config for the embedding network
  value:
    name: UNet2D
    in_channels: 4
    out_channels: 16
    # use Groupnorm instead of Batchnorm for DSB; Batchnorm introduces artifacts around nuclei due to the difference
    # in intensity distribution between images with large and small cells
    layer_order: bcr
    num_groups: 8
    f_maps: [32, 64, 128]
    #conv_padding: 0
    final_sigmoid: false
    is_segmentation: false

gnn_n_hl:
  desc: number of hidden layers in graph conv blocks
  value: 1
gnn_size_hl:
  desc: factor by which feature size of hidden featuremaps in gnn increases/decreases
  value: 2028
gnn_act_depth:
  desc: num of graph convs (1, 2, 3)
  value: 1
gnn_act_norm_inp:
  desc: put hidden features on unit sphere and scale by sim
  value: false
gnn_crit_depth:
  desc: num of graph convs (1, 2)
  value: 1
gnn_crit_norm_inp:
  desc: put hidden features on unit sphere and scale by sim
  value: false
gnn_dropout:
  desc: dropout probability during training \in [0, 1]
  value: 0.0

# training config
T_max:
  desc: Number of training steps
  value: 10
mem_size:
  desc: Mem size in replay memory buffer
  value: 3
data_update_frequency:
  desc: update env data every n steps
  value: 3
post_stats_frequency:
  desc: log every n steps
  value: 10
validatoin_freq:
  desc: validate every n steps
  value: 3
store_indices:
  desc: sample indices of validation set for samples that are logged at each validation step
  value: [1, 2, 4, 5]
n_updates_per_step:
  desc: perform n optim steps per env step
  value: 2
n_explorers:
  desc: n experience explorer threads
  value: 1
n_min_expl_before_opt:
  desc: minimum number of explorer steps before one trainer step
  value: 1
batch_size:
  desc: num samples in minibatch
  value: 4

# conf for lr scheduling
lr_sched:
  desc: config for moving averages and learning rate sheduling
  value:
    mov_avg_bandwidth: 50
    weight_range: [0.1, 1.0]
    step_frequency: 10
    mov_avg_offset: 0.0
    torch_sched:  # conf for torch reduceOnPlateau scheduler
      patience: 10000
      min_lr: 0.00001
      factor: 0.05
      threshold: 0.01

# specific configs for RL
reward_function:
  desc: one of SubGraphDiceReward, HoughCirclesReward, or self defined in ./rewards
  value: HoughCirclesReward
s_subgraph:
  desc: subgraph sizes
  value: [4, 6, 12, 64] # [8, 16, 32, 64]
entropy_range:
  desc: range of the min entropy constraint
  value: [0.05, 2.0]
init_temperature:
  desc: initial temperature in Gibbs distribution
  value: 0.1
alpha_lr:
  desc: learning rate for alpha opt
  value: 0.0001
actor_lr:
  desc: learning rate for actor opt
  value: 0.0001
critic_lr:
  desc: learning rate for critic opr
  value: 0.0001
actor_update_frequency:
  desc: optim every n-th step
  value: 1
actor_update_after:
  desc: update actor after critic has been warmed up
  value: 1
critic_tau:
  desc: update weight for target update
  value: 0.005
critic_target_update_frequency:
  desc: update critic target every nth step
  value: 5

use_closed_form_entropy:
  desc: since we have normal distributions we can use a closed for entropy
  value: true
side_loss_weight:
  desc: loss weight for the edge-gnn side loss
  value: 0
# specification of multinomial gaussian, acting as policy
std_bounds:
  desc: norm bounds of predicted std dev
  value: [0.01, 5.0]
mu_bounds:
  desc: norm bounds of predicted mean
  value: [-5, 5]
