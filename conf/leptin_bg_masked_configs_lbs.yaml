# general configs
verbose:
  desc: prints intermediate results to sdt.out
  value: true
run_id:
  desc: dir name in base-dir. Logs are in here
  value: mixed_prec
data_dir:
  desc: train data dir
  value: /g/kreshuk/hilt/projects/data/leptin_fused_tp1_ch_0/train/bg_masked_data
val_data_dir:
  desc: validation data dir
  value: /g/kreshuk/hilt/projects/data/leptin_fused_tp1_ch_0/val/bg_masked_data
agent_model_name:
  desc: if not empty, agent will be initialized with the state dict at this location
  value: '' # agent_model_svpfe.pth

# embedding network config
fe_model_name:
  desc: path to state dict of trained model
  value: /g/kreshuk/hilt/storage/leptin_data_nets/fe_cosine_ls_bn.pth
dim_embeddings:
  desc: number of embedding feature channels
  value: 16
distance:
  desc: used distance in embedding space, can be l2 or cosine
  value: cosine
backbone:
  desc: config for the embedding network
  value:
    name: UNet2D
    in_channels: 1
    out_channels: 16
    # use Groupnorm instead of Batchnorm for DSB; Batchnorm introduces artifacts around nuclei due to the difference
    # in intensity distribution between images with large and small cells
    layer_order: bcr
    num_groups: 8
    f_maps: [32, 64, 128, 256]
    #conv_padding: 0
    final_sigmoid: false
    is_segmentation: false

gnn_n_hidden:
  desc: number of hidden layers in graph conv blocks
  value: 2
gnn_hl_factor:
  desc: factor by which feature size of hidden featuremaps in gnn increases/decreases
  value: 160
gnn_act_depth:
  desc: num of graph convs (1, 2, 3)
  value: 1
gnn_act_norm_inp:
  desc: put hidden features on unit sphere and scale by sim
  value: false
gnn_crit_depth:
  desc: num of graph convs (1, 2)
  value: 1
gnn_crit_norm_inp:
  desc: put hidden features on unit sphere and scale by sim
  value: true

# training config
T_max:
  desc: Number of training steps
  value: 50000
mem_size:
  desc: Mem size in replay memory buffer
  value: 100
data_update_frequency:
  desc: update env data after n steps
  value: 100
post_stats_frequency:
  desc: post logs after n steps
  value: 10
validatoin_freq:
  desc: validate after n steps
  value: 500
n_updates_per_step:
  desc: perform n optim steps per env step
  value: 10
n_explorers:
  desc: n experience explorer threads
  value: 4
batch_size:
  desc: num samples in minibatch
  value: 4

# conf for lr scheduling
lr_sched:
  desc: config for moving averages and learning rate sheduling
  value:
    mov_avg_bandwidth: 40
    weight_range: [0.1, 1.0]
    step_frequency: 10
    mov_avg_offset: 0.0
    torch_sched:  # conf for torch reduceOnPlateau scheduler
      patience: 100
      min_lr: 0.0001
      factor: 0.05
      threshold: 0.01

patch_manager:
  desc: if training should happen on patches, this is the config for the patch manager
  value:
    name: none # rotated, no_cross, none
    reorder_sp: false


# specific configs for the sac algorithm
reward_function:
  desc: one of sub_graph_dice, leptin_dataTurningFunc, leptin_dataEllipticFit, leptin_dataTurningWithEllipses
  value: leptin_dataTurningWithEllipses
gt_edge_overlap_thresh:
  desc: if using gt edges this is the overlap thresh of the underlying gt for a gt edge to be 1
  value: 0.5
s_subgraph:
  desc: subgraph sizes
  value: [2, 4, 6] # [8, 16, 32, 64]
entropy_range:
  desc: range of the min entropy constraint
  value: [0.05, 2.0]
init_temperature:
  desc: initial temperature in Gibbs distribution
  value: 0.1
alpha_lr:
  desc: learning rate for alpha opt
  value: 0.03921
actor_lr:
  desc: learning rate for actor opt
  value: 0.001356
actor_update_frequency:
  desc: optim every n-th step
  value: 1
actor_update_after:
  desc: update actor after critic has been warmed up
  value: 1
critic_lr:
  desc: learning rate for critic opr
  value: 0.02467
critic_tau:
  desc: update weight for target update
  value: 0.005
critic_target_update_frequency:
  desc: update critic target every nth step
  value: 5

use_closed_form_entropy:
  desc: since we have normal distributions we can use a closed for entropy
  value: true
side_loss_weight:
  desc: loss weight for the edge-gnn side loss
  value: 0
# specification of multinomial gaussian, acting as policy
std_bounds:
  desc: norm bounds of predicted std dev
  value: [0.01, 5.0]
mu_bounds:
  desc: norm bounds of predicted mean
  value: [-5, 5]

#policy_warmup:
#  desc: the policy might need pretraining to make training faster
#    value:
#    lr: 1e-3
#
#    n_iterations: 500 # number of iterations of feature extrqactor warmup
#    batch_size: 1  # batch size for feature extractor warmup
#
#    patch_manager:
#      name: no_cross # rotated, no_cross, none
#      patch_shape: [512, 512]
#      patch_stride: [128, 128]
#      reorder_sp: true

